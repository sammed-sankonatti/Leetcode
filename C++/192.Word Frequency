# Read from the file words.txt and output the word frequency list to stdout.

cat words.txt | tr -s ' ' '\n' | sort | uniq --count | sort -r | awk '{print $2 " " $1}'

cat words.txt | tr $'\n' ' ' | sed -r "s/ +/\n/g" | sort | uniq -c | sed "s/^[ \t]*//" | cut -d ' ' -f 1,2 | sort -rn | sed -r "s/([0-9]+) ([a-zA-Z]+)/\2 \1/" 

cat words.txt |tr ' ' '\n' | sort | uniq -c | sort -k1nr | awk '{print$2" "$1}'

xargs -n1 < words.txt | sort | uniq -c | sort -r | awk '{print $2" "$1}' 


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
Method 2 - associative array in bash script (16ms)

declare -A arr #associative array

while IFS= read -r line
do
    for word in $line
    do
        let arr[$word]=${arr[$word]}+1
    done
done < words.txt

for key in ${!arr[@]}
do
    echo $key ${arr[$key]}
done | sort -rn -k2